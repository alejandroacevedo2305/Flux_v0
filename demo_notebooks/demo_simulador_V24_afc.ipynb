{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#os.chdir(\"/DeepenData/Repos/Flux_v0\")\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "\n",
    "import os\n",
    "from datetime import date\n",
    "\n",
    "import pandas as pd  # Dataframes\n",
    "import polars as pl  # DFs, es más rapido\n",
    "from pydantic import BaseModel, Field, ConfigDict  # Validacion de datos\n",
    "from datetime import date, datetime\n",
    "\n",
    "import releases.simv7 as sim\n",
    "\n",
    "\n",
    "from datetime import date, datetime, time\n",
    "from functools import lru_cache\n",
    "\n",
    "import pandas as pd  # Dataframes\n",
    "import polars as pl  # DFs, es más rapido\n",
    "from fastapi import HTTPException\n",
    "from pydantic import BaseModel, ConfigDict, Field  # Validacion de datos\n",
    "\n",
    "#from ..modelos_datos.api import Serie\n",
    "\n",
    "class Serie(BaseModel):\n",
    "    \"\"\"\n",
    "    Describe la info para una serie\n",
    "\n",
    "    - `IdSerie` : identificador numerico de la serie, acorde a las DB\n",
    "    - `serie` : nombre de la serie. `''` por default.\n",
    "    - `tiempo_maximo_espera` : tiempo de espera para el corte de SLA, en segundos. `600` (10min) por default.\n",
    "    - `sla_objetivo` : corte de  SLA. Usado para visualizaciones, pero será necesario para el optimizador. Default `0.80` (80%).\n",
    "    \"\"\"\n",
    "\n",
    "    IdSerie: int\n",
    "    serie: str = \"\"\n",
    "    tiempo_maximo_espera: int = 600  # En segundos\n",
    "    sla_objetivo: float = 0.80\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(f\"{self.IdSerie}, {self.serie}, {self.tiempo_maximo_espera}\")\n",
    "\n",
    "\n",
    "class DatasetTTP(BaseModel):\n",
    "    \"\"\"\n",
    "    Tablas de atenciones y configuraciones\n",
    "\n",
    "    Un wrapper que permite consultar una base de datos sobre las atenciones y configuraciones de una oficina.\n",
    "\n",
    "    ## Modo de uso\n",
    "    >>> # Define parametros de inicializacion\n",
    "    >>> ID_OFICINA = 2\n",
    "    >>> DB_CONN = \"mysql://<user>:<pass>@<database>:<port>/<database>\" # Modifica con valores reales\n",
    "    ...\n",
    "    >>> dataset = DatasetTTP(connection_string=DB_CONN, id_oficina=ID_OFICINA)\n",
    "    ...\n",
    "    >>> atenciones, planificacion = dataset.un_dia(fecha=FECHA)\n",
    "    >>> atenciones\n",
    "    \"\"\"\n",
    "\n",
    "    # Parametros de instanciacion, esto no llena de data hasta llamar otros metodos\n",
    "    connection_string: str = Field(description=\"Un string de conexion a una base de datos\")\n",
    "    id_oficina: int = Field(description=\"Identificador numerico de una oficina\")\n",
    "\n",
    "    # FIXME: esto existe porque la clase no esta definida como un tipo valido\n",
    "    # (quiero tipos porque atrapan errores antes de que se propaguen)\n",
    "    model_config = ConfigDict(arbitrary_types_allowed=True)\n",
    "\n",
    "    # Componentes que deben poder entrar inmediatamente al simulador\n",
    "    atenciones: pd.DataFrame | pl.DataFrame = None  # TODO: reemplazar por Pandera\n",
    "    planificacion: dict = None  # TODO: reemplazar por Pydantic\n",
    "    configuraciones: pd.DataFrame | pl.DataFrame = None  # Post-inicializado\n",
    "\n",
    "    # Fechas que se extiende el dataset\n",
    "    FH_min: datetime = None\n",
    "    FH_max: datetime = None\n",
    "\n",
    "    # Fechas que se extiende el dataset cargado\n",
    "    atenciones_FH_min: datetime = None\n",
    "    atenciones_FH_max: datetime = None\n",
    "\n",
    "    # Mete todo en un caché para este objeto, para asi tener que es cacheable o no\n",
    "    def __hash__(self):\n",
    "        return hash(f\"{self.id_oficina}, {self.FH_max}\")  # {self.atenciones_FH_max},{self.atenciones_FH_min}\")\n",
    "\n",
    "    def model_post_init(self, _) -> None:\n",
    "        # Llama a la ultima configuracion de la oficina\n",
    "        self.configuraciones = pl.read_database_uri(\n",
    "            uri=self.connection_string,\n",
    "            query=f\"\"\"\n",
    "            SELECT * FROM Configuraciones WHERE (IdOficina = {self.id_oficina});\n",
    "            \"\"\",\n",
    "        ).sort(by=[\"IdEsc\", \"prioridad\"])\n",
    "\n",
    "        # Determina los rangos en que se puede llamar este dataset\n",
    "        self.FH_min, self.FH_max = (\n",
    "            pl.read_database_uri(\n",
    "                uri=self.connection_string,\n",
    "                query=\"SELECT min(FH_Emi) AS min, max(FH_Emi) AS max FROM Atenciones;\",\n",
    "            )\n",
    "            .to_dicts()[0]\n",
    "            .values()\n",
    "        )\n",
    "\n",
    "    # Metodos para rellenar atenciones y planificacion\n",
    "    def forecast(self):\n",
    "        # Conectare la logica luego. La idea es poder dejar los forecast en la base de datos,\n",
    "        # para pre-calcularlos, dado que eso demora unos minutos y es algo tedioso\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def un_dia(self, fecha: date) -> tuple[pd.DataFrame, dict]:\n",
    "        \"\"\"\n",
    "        Modifica las `atenciones` y `planificacion` a una fecha historica\n",
    "\n",
    "        Usa una fecha en un formato reconocible por Pandas, devuelve error en caso de no\n",
    "        tener atenciones para ese dia. Retorna la tabla de atenciones y el diccionario de\n",
    "        planificacion para el dia, y modifica esto en el objeto `self` para no tener que\n",
    "        ejecutar esta funcion de nuevo para un mismo dia.\n",
    "        \"\"\"\n",
    "        # PARTE TRIVIAL, LEE LAS ATENCIONES DE UNA FECHA\n",
    "        fecha = pd.Timestamp(fecha)  # Converte la fecha a un timestamp\n",
    "\n",
    "        # NOTA: el query llama a las columnas por nombre para asi verificar que estas están\n",
    "        # en el schema de la base de datos, else, se rompe por un error de MySQL.\n",
    "        self.atenciones = pl.read_database_uri(\n",
    "            uri=self.connection_string,\n",
    "            query=f\"\"\"\n",
    "            SELECT \n",
    "                IdOficina, -- Sucursal fisica\n",
    "                IdSerie,   -- Motivo de atencion\n",
    "                IdEsc,     -- Escritorio de atencion\n",
    "                FH_Emi,    -- Emision del tiquet, con...\n",
    "                FH_Llama,  -- ...hora de llamada a resolverlo,...\n",
    "                FH_AteIni, -- ...inicio de atencion, y...\n",
    "                FH_AteFin, -- ...termino de atencion\n",
    "                t_esp,     -- Tiempo de espera    \n",
    "                t_ate      -- Tiempo de atención\n",
    "            FROM \n",
    "                Atenciones\n",
    "            WHERE\n",
    "                (IdOficina = {self.id_oficina} ) AND -- Selecciona solo una oficina, segun arriba\n",
    "                (FH_Emi > '{fecha}') AND (FH_Emi < '{fecha + pd.Timedelta(days=1)}') -- solo el dia\n",
    "\n",
    "            ORDER BY FH_Emi DESC -- Ordenado de mas reciente hacia atras (posiblemente innecesario);\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "        # NOTA: esto seria solo .empty (atributo) en un pd.DataFrame, pero Polars es mas rapido\n",
    "        if self.atenciones.is_empty():\n",
    "            raise Exception(\"Tabla de atenciones vacia\", f\"Fecha sin atenciones: {fecha}\")\n",
    "\n",
    "        # Update del caché de atenciones\n",
    "        self.atenciones_FH_min = self.atenciones[\"FH_Emi\"].min()\n",
    "        self.atenciones_FH_max = self.atenciones[\"FH_Emi\"].max()\n",
    "\n",
    "        # PRIORIDADES DE SERIES, COMPATIBLE CON INFERIDAS Y GUARDADAS EN CONFIG\n",
    "        lista_config = (\n",
    "            # Lista de prioridades por serie, en la configuracion ultima\n",
    "            self.configuraciones.group_by(by=[\"IdSerie\"])  # BUG: el linter no reconoce esto correctamente\n",
    "            .agg(pl.mean(\"prioridad\"), pl.count(\"IdEsc\"))\n",
    "            .sort(by=[\"prioridad\", \"IdEsc\"], descending=[False, True])[\"IdSerie\"]\n",
    "            .to_list()\n",
    "        )\n",
    "\n",
    "        lista_atenciones = (\n",
    "            # Usa un contador de atenciones para rankear mas arriba con mas atenciones\n",
    "            self.atenciones[\"IdSerie\"].value_counts().sort(by=\"counts\", descending=True)[\"IdSerie\"].to_list()\n",
    "        )\n",
    "\n",
    "        # Esto genera una lista global de prioridades, donde todo lo demas puede ir a la cola\n",
    "        map_prioridad = {\n",
    "            id_serie: rank + 1\n",
    "            for rank, id_serie in enumerate(\n",
    "                # Combina ambas listas, prefiriendo la de la configuracion ultima sobre la inferida del dia\n",
    "                [s for s in lista_config if (s in lista_atenciones)]\n",
    "                + [s for s in lista_atenciones if (s not in lista_config)]\n",
    "            )\n",
    "        }\n",
    "\n",
    "        # TODO: Esta parte genera una lista de prioridades en la configuracion del dia,\n",
    "        # resolviendo incompatibilidades de que algo no esté originalmente.\n",
    "        df_configuraciones = (\n",
    "            self.atenciones.select([\"IdEsc\", \"IdSerie\"])\n",
    "            .unique(keep=\"first\")\n",
    "            .sort(by=[\"IdEsc\", \"IdSerie\"], descending=[True, True])\n",
    "            .with_columns(prioridad=pl.col(\"IdSerie\").replace(map_prioridad, default=None))\n",
    "        )\n",
    "\n",
    "        # Esta es la tabla de metadata para las series, por separado por comprension\n",
    "        df_series_meta = (\n",
    "            self.configuraciones.select([\"IdSerie\", \"serie\", \"tiempo_maximo_espera\"])\n",
    "            .unique(keep=\"first\")\n",
    "            .with_columns(prioridad=pl.col(\"IdSerie\").replace(map_prioridad, default=None))\n",
    "        )\n",
    "\n",
    "        # Se unifica con la de configuracion diaria\n",
    "        df_configuraciones = df_configuraciones.join(df_series_meta, on=\"IdSerie\")\n",
    "\n",
    "        # EMPIEZA A CONSTRUIR LA PLANIFICACION DESDE HORARIOS Y ESCRITORIOS\n",
    "        horarios = self.atenciones.group_by(by=[\"IdEsc\"]).agg(\n",
    "            inicio=pl.min(\"FH_Emi\").dt.round(\"1h\"),\n",
    "            termino=pl.max(\"FH_AteIni\").dt.round(\"1h\"),\n",
    "        )\n",
    "\n",
    "        self.planificacion = {\n",
    "            e[\"IdEsc\"]: [\n",
    "                {\n",
    "                    \"inicio\": str(e[\"inicio\"].time()),\n",
    "                    \"termino\": str(e[\"termino\"].time()),\n",
    "                    \"propiedades\": {\n",
    "                        # Esta parte saca los skills comparando con el dict de configs\n",
    "                        \"skills\": df_configuraciones.filter(pl.col(\"IdEsc\") == e[\"IdEsc\"])[\"IdSerie\"].to_list(),\n",
    "                        # asumimos que la configuracion de todos es rebalse\n",
    "                        # TODO: el porcentaje de actividad es mas complicado, asumire un 80%\n",
    "                        \"configuracion_atencion\": \"Rebalse\",\n",
    "                        \"porcentaje_actividad\": 0.80,\n",
    "                        # Esto es innecesariamente nested\n",
    "                        \"atributos_series\": [\n",
    "                            {\n",
    "                                \"serie\": s[\"IdSerie\"],\n",
    "                                \"sla_porcen\": 80,  # Un valor por defecto\n",
    "                                \"sla_corte\": s[\"tiempo_maximo_espera\"],\n",
    "                                \"pasos\": 1,  # Un valor por defecto\n",
    "                                \"prioridad\": s[\"prioridad\"],\n",
    "                            }\n",
    "                            for s in df_configuraciones.filter(pl.col(\"IdEsc\") == e[\"IdEsc\"]).to_dicts()\n",
    "                        ],\n",
    "                    },\n",
    "                }\n",
    "            ]\n",
    "            for e in horarios.to_dicts()\n",
    "        }\n",
    "\n",
    "        # No usamos Polars depues de esto\n",
    "        if True:  # not DF_POLARS:\n",
    "            self.atenciones = self.atenciones.to_pandas()\n",
    "\n",
    "        return self.atenciones, self.planificacion\n",
    "\n",
    "    @lru_cache  # Requiere que todo sea cacheable\n",
    "    def promedio_n_semanas(\n",
    "        self,\n",
    "        fecha: date,\n",
    "        series: tuple[Serie],\n",
    "        inicio=\"08:00\",\n",
    "        termino=\"18:00\",\n",
    "        tiempo_maximo_expera: int = 600,\n",
    "        semanas: int = 6,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Genera una aproximacion base de una prediccion de demanda y tiempos de espera\n",
    "\n",
    "        Los promedios de las ultimas seis semanas son adecuados en muchos casos de flujo constante,\n",
    "        y son triviales de computar. Sobre esta, la prediccion puede ser complementada con SARIMAX o\n",
    "        XGBoost en caso de tener data suficiente y una estacionalidad adecuada (osea series populares).\n",
    "\n",
    "        >>> dataset = DatasetTTP(uri_database, database)\n",
    "        >>> dataset.ultimas_n_semanas(fecha, n_semanas)\n",
    "        >>> response = promedio_desde_semanas( dataset.atenciones, series )\n",
    "        \"\"\"\n",
    "\n",
    "        fecha = pd.Timestamp(fecha)  # Converte la fecha a un timestamp\n",
    "        fecha_prev = fecha - pd.Timedelta(weeks=semanas)\n",
    "\n",
    "        dia_semana = pd.Timestamp(fecha).weekday() + 1\n",
    "\n",
    "        self.atenciones = pl.read_database_uri(\n",
    "            uri=self.connection_string,\n",
    "            query=f\"\"\"\n",
    "            SELECT \n",
    "                IdOficina, -- Sucursal fisica\n",
    "                IdSerie,   -- Motivo de atencion\n",
    "                IdEsc,     -- Escritorio de atencion\n",
    "                FH_Emi,    -- Emision del tiquet, con...\n",
    "                FH_Llama,  -- ...hora de llamada a resolverlo,...\n",
    "                FH_AteIni, -- ...inicio de atencion, y...\n",
    "                FH_AteFin, -- ...termino de atencion\n",
    "                t_esp,     -- Tiempo de espera    \n",
    "                t_ate      -- Tiempo de atención\n",
    "            FROM \n",
    "                Atenciones\n",
    "            WHERE\n",
    "                (IdOficina = {self.id_oficina} ) AND -- Selecciona solo una oficina, segun arriba\n",
    "                (FH_Emi > '{fecha_prev}') AND (FH_Emi < '{fecha + pd.Timedelta(days=1)}') -- solo el dia\n",
    "\n",
    "            ORDER BY FH_Emi DESC -- Ordenado de mas reciente hacia atras (posiblemente innecesario);\n",
    "            \"\"\",\n",
    "        )\n",
    "\n",
    "        # Update del caché de atenciones\n",
    "        self.atenciones_FH_min = self.atenciones[\"FH_Emi\"].min()\n",
    "        self.atenciones_FH_max = self.atenciones[\"FH_Emi\"].max()\n",
    "\n",
    "        # def promedio_desde_semanas(\n",
    "        #     self,\n",
    "        #     fecha: date,\n",
    "        #     atenciones: pd.DataFrame | pl.DataFrame,  # type: ignore\n",
    "        #     series: tuple[Serie],\n",
    "        #     tiempo_maximo_expera: int = 600,\n",
    "        # ):\n",
    "\n",
    "        # Desempaca las series en tiempos de espera\n",
    "        # nota que el \"tiempo_maximo_espera\" es el de la Oficina\n",
    "        tiempos_espera = {int(serie.IdSerie): int(serie.tiempo_maximo_espera) for serie in series}\n",
    "        id_series = [int(serie.IdSerie) for serie in series] + [0]\n",
    "\n",
    "        # Converite el dataframe de Pandas en uno de Polars\n",
    "        # if isinstance(self.atenciones, pd.DataFrame):\n",
    "        #     atenciones = pl.from_pandas(atenciones)\n",
    "\n",
    "        # LazyDataFrame, lo evaluamos hata el final\n",
    "        atenciones: pl.LazyFrame = (\n",
    "            self.atenciones.lazy()\n",
    "            .filter(pl.col(\"IdSerie\").is_in(tiempos_espera.keys()))\n",
    "            .with_columns(\n",
    "                # Columnas para cortes SLA por serie, y corte SLA de oficina\n",
    "                cumple_sla=pl.col(\"t_esp\").le(pl.col(\"IdSerie\").replace(tiempos_espera)),\n",
    "                cumple_sla_glob=pl.col(\"t_esp\").le(tiempo_maximo_expera),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # ======================================================================\n",
    "        # AGRUPACION POR HORA DE LAS ATENCIONES - Por Serie\n",
    "        atenciones_series = atenciones.group_by(\n",
    "            [\n",
    "                pl.col(\"FH_Emi\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"FH_Emi\").dt.hour().alias(\"hora\"),\n",
    "                pl.col(\"IdSerie\").cast(pl.Int32),\n",
    "            ]\n",
    "        ).agg(\n",
    "            # Agregados\n",
    "            pl.col(\"t_ate\").median().cast(pl.UInt32),\n",
    "            pl.col(\"t_esp\").median().cast(pl.UInt32),\n",
    "            pl.col(\"IdOficina\").count().alias(\"n_ate\"),\n",
    "            pl.col(\"IdEsc\").n_unique().alias(\"n_esc\"),\n",
    "            pl.col(\"cumple_sla\").mean().alias(\"sla\"),\n",
    "        )\n",
    "\n",
    "        # AGRUPACION POR HORA DE LAS ATENCIONES - Total\n",
    "        atenciones_id_0 = (\n",
    "            atenciones.group_by(\n",
    "                [\n",
    "                    pl.col(\"FH_Emi\").dt.weekday().alias(\"weekday\"),\n",
    "                    pl.col(\"FH_Emi\").dt.hour().alias(\"hora\"),\n",
    "                ]\n",
    "            )\n",
    "            .agg(\n",
    "                # Agregados\n",
    "                pl.col(\"t_ate\").median().cast(pl.UInt32),\n",
    "                pl.col(\"t_esp\").median().cast(pl.UInt32),\n",
    "                pl.col(\"IdOficina\").count().alias(\"n_ate\"),\n",
    "                pl.col(\"IdEsc\").n_unique().alias(\"n_esc\"),\n",
    "                pl.col(\"cumple_sla_glob\").mean().alias(\"sla\"),\n",
    "            )\n",
    "            .with_columns(IdSerie=0)\n",
    "        )\n",
    "\n",
    "        atenciones_por_hora = (\n",
    "            pl.concat([atenciones_series, atenciones_id_0], how=\"align\")\n",
    "            .sort(by=[\"weekday\", \"IdSerie\", \"hora\"])\n",
    "            .filter(pl.col(\"weekday\") == dia_semana)\n",
    "            .with_columns(\n",
    "                pl.col(\"hora\")\n",
    "                .map_elements(lambda x: str(pd.Timestamp(fecha) + pd.Timedelta(x, unit=\"H\")), return_dtype=str)\n",
    "                .str.to_datetime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                .alias(\"FH\")\n",
    "            )\n",
    "            .drop([\"weekday\", \"hora\"])\n",
    "        )\n",
    "\n",
    "        # ======================================================================\n",
    "        # REINDEXADO\n",
    "        reindex_df = (\n",
    "            pd.MultiIndex.from_product(\n",
    "                iterables=[\n",
    "                    pd.date_range(f\"{fecha} {inicio}\", f\"{fecha} {termino}\", freq=\"H\").round(\"H\"),\n",
    "                    [0] + [s.IdSerie for s in series],\n",
    "                ],\n",
    "                names=[\"FH\", \"IdSerie\"],\n",
    "            )\n",
    "            .to_frame(index=False)\n",
    "            .sort_values(by=[\"IdSerie\", \"FH\"])\n",
    "        )\n",
    "\n",
    "        # MultiIndex([('2023-12-22 08:00:00', 0),\n",
    "        #             ('2023-12-22 08:00:00', 1),\n",
    "        #             ('2023-12-22 08:00:00', 5),\n",
    "        #             ('2023-12-22 09:00:00', 0),\n",
    "        #             ...\n",
    "\n",
    "        atenciones_por_hora = (\n",
    "            pl.from_pandas(reindex_df)\n",
    "            .lazy()\n",
    "            .cast({\"FH\": pl.Datetime, \"IdSerie\": pl.Int32})\n",
    "            .join(\n",
    "                atenciones_por_hora,\n",
    "                on=[\"IdSerie\", \"FH\"],\n",
    "                how=\"left\",\n",
    "            )\n",
    "            .with_columns(\n",
    "                # Reemplaza los blancos\n",
    "                pl.col(\"t_ate\").fill_null(0),\n",
    "                pl.col(\"t_esp\").fill_null(0),\n",
    "                pl.col(\"n_ate\").fill_null(0),\n",
    "                pl.col(\"n_esc\").fill_null(0),\n",
    "                pl.col(\"sla\").fill_null(1),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # ======================================================================\n",
    "        # CUMULATIVA DE LAS ATENCIONES - Por Serie\n",
    "        atenciones_c_series = atenciones.group_by(\n",
    "            [\n",
    "                pl.col(\"FH_Emi\").dt.weekday().alias(\"weekday\"),\n",
    "                pl.col(\"IdSerie\").cast(pl.Int32),\n",
    "            ]\n",
    "        ).agg(\n",
    "            # Agregados\n",
    "            pl.col(\"t_ate\").median().cast(pl.UInt32),\n",
    "            pl.col(\"t_esp\").median().cast(pl.UInt32),\n",
    "            pl.col(\"IdOficina\").count().alias(\"n_ate\"),\n",
    "            pl.col(\"IdEsc\").n_unique().alias(\"n_esc\"),\n",
    "            pl.col(\"cumple_sla\").mean().alias(\"sla\"),\n",
    "        )\n",
    "\n",
    "        # CUMULATIVA DE LAS ATENCIONES - Total\n",
    "        atenciones_c_id_0 = (\n",
    "            atenciones.group_by(\n",
    "                [\n",
    "                    pl.col(\"FH_Emi\").dt.weekday().alias(\"weekday\"),\n",
    "                ]\n",
    "            )\n",
    "            .agg(\n",
    "                # Agregados\n",
    "                pl.col(\"t_ate\").median().cast(pl.UInt32),\n",
    "                pl.col(\"t_esp\").median().cast(pl.UInt32),\n",
    "                pl.col(\"IdOficina\").count().alias(\"n_ate\"),\n",
    "                pl.col(\"IdEsc\").n_unique().alias(\"n_esc\"),\n",
    "                pl.col(\"cumple_sla_glob\").mean().alias(\"sla\"),\n",
    "            )\n",
    "            .with_columns(IdSerie=0)\n",
    "        )\n",
    "\n",
    "        atenciones_cumulativo = (\n",
    "            pl.concat([atenciones_c_series, atenciones_c_id_0], how=\"align\")\n",
    "            .sort(by=[\"weekday\", \"IdSerie\"])\n",
    "            .filter(pl.col(\"weekday\") == dia_semana)\n",
    "            .drop([\"weekday\"])\n",
    "        )\n",
    "\n",
    "        # NOTA: Puede ocurrir que si una serie no tiene atenciones, no se vea reflejada en la suma total\n",
    "        # de atenciones durante el dia. Esto es corregido usando el modelo validador de SimOutput.\n",
    "\n",
    "        return {\n",
    "            \"sla_instantaneo\": atenciones_por_hora.collect().to_dicts(),\n",
    "            \"sla_cumulativo\": atenciones_cumulativo.collect().to_dicts(),  # sla_cumulativo + extras,\n",
    "            \"uso_escritorios\": [],\n",
    "        }\n",
    "\n",
    "    # METADATA\n",
    "    __version__ = 2.0  # El otro es la version original. Este REQUIERE una db."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 258)\n",
      "(2, 239)\n",
      "(3, 214)\n",
      "(4, 93)\n",
      "(5, 214)\n",
      "(6, 289)\n",
      "(7, 156)\n",
      "(8, 245)\n",
      "(9, 125)\n",
      "(10, 96)\n",
      "(11, 168)\n",
      "(12, 107)\n",
      "(13, 106)\n",
      "(14, 168)\n",
      "(15, 160)\n",
      "(16, 196)\n",
      "(17, 305)\n",
      "(18, 95)\n",
      "(19, 81)\n",
      "(20, 151)\n",
      "(21, 36)\n",
      "(22, 250)\n",
      "(25, 42)\n",
      "(26, 45)\n",
      "(27, 36)\n",
      "(28, 128)\n",
      "(29, 47)\n",
      "(30, 92)\n",
      "(31, 95)\n",
      "(32, 77)\n",
      "(33, 86)\n",
      "(34, 142)\n",
      "(35, 120)\n",
      "(36, 106)\n",
      "(37, 32)\n",
      "(38, 84)\n",
      "(39, 131)\n",
      "(40, 63)\n",
      "(41, 142)\n",
      "(42, 60)\n",
      "(43, 79)\n",
      "(44, 48)\n",
      "(45, 69)\n",
      "(46, 39)\n",
      "(47, 48)\n",
      "(48, 75)\n",
      "(49, 41)\n",
      "(50, 55)\n",
      "(51, 432)\n",
      "(53, 566)\n",
      "(54, 89)\n",
      "(55, 28)\n"
     ]
    }
   ],
   "source": [
    "FECHA = \"2023-12-15\"\n",
    "ID_OFICINA = 55\n",
    "DATABASE_URI: str = \"mysql://autopago:Ttp-20238270@totalpackmysql.mysql.database.azure.com:3306/\"\n",
    "ID_DATABASE = \"capacity_data_afc\"\n",
    "ID_OFICINA_ok = []\n",
    "for ID_OFICINA in range(60):\n",
    "    try:\n",
    "        dataset = DatasetTTP(connection_string=DATABASE_URI + ID_DATABASE, id_oficina=ID_OFICINA)\n",
    "        el_dia_real, plan  = dataset.un_dia(fecha=FECHA)\n",
    "        print((ID_OFICINA,el_dia_real.shape[0]))\n",
    "        ID_OFICINA_ok.append((ID_OFICINA,el_dia_real.shape[0]))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "        #print(f\"NO válido ID_OFICINA {ID_OFICINA}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53, 51, 17, 6, 1]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data      = sorted(ID_OFICINA_ok, key=lambda x: x[1], reverse=True)\n",
    "ids_oficinas_top =  [i[0] for i in sorted_data[:5]]\n",
    "ids_oficinas_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "simulaciones = []\n",
    "for ID_OFICINA in ids_oficinas_top:\n",
    "\n",
    "\n",
    "    dataset = DatasetTTP(connection_string=DATABASE_URI + ID_DATABASE, id_oficina=ID_OFICINA)\n",
    "    el_dia_real, plan  = dataset.un_dia(fecha=FECHA)\n",
    "    el_dia_real['T_Ate'] = (el_dia_real['FH_AteFin'] - el_dia_real['FH_AteIni']).astype('timedelta64[s]').dt.total_seconds().astype(int)\n",
    "    el_dia_real['T_Esp'] = (el_dia_real['FH_AteIni'] - el_dia_real['FH_Emi']).astype('timedelta64[s]').dt.total_seconds().astype(int)\n",
    "    el_dia_real = el_dia_real.sort_values(by='FH_Emi', inplace=False).astype(\n",
    "        {\n",
    "            'FH_Emi': 'datetime64[s]',\n",
    "            'FH_Llama': 'datetime64[s]',\n",
    "            'FH_AteIni': 'datetime64[s]',\n",
    "            'FH_AteFin': 'datetime64[s]',})\n",
    "    ######################\n",
    "    #------Simulacion-----\n",
    "    ######################\n",
    "    import time\n",
    "    start_time           = time.time()\n",
    "    hora_cierre          = \"17:30:00\"\n",
    "    porcentaje_actividad =.8\n",
    "    registros_atenciones_simulacion, fila = sim.simv7_1(\n",
    "                                                        el_dia_real, hora_cierre, \n",
    "                                                        #plan,\n",
    "                                                        sim.plan_desde_skills(skills=sim.obtener_skills(el_dia_real) , \n",
    "                                                                            inicio = '08:00:00', \n",
    "                                                                            porcentaje_actividad=porcentaje_actividad),\n",
    "                                                        probabilidad_pausas =  0.85,        # \n",
    "                                                        factor_pausas       = .065,         #\n",
    "                                                        params_pausas       =  [0, 1/2, 1] ,#\n",
    "                                                        \n",
    "                                                    )\n",
    "        #, log_path=\"dev/simulacion.log\")\n",
    "    print(f\"{len(registros_atenciones_simulacion) = }, {len(fila) = }\")\n",
    "    end_time = time.time()\n",
    "    print(f\"tiempo total: {end_time - start_time:.1f} segundos\")\n",
    "    simulaciones.append(registros_atenciones_simulacion)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sim.compare_historico_vs_simulacion(el_dia_real, registros_atenciones_simulacion,  ID_DATABASE, ID_OFICINA,FECHA ,porcentaje_actividad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ----fin------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
